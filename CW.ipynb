{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle('../MIMIC40/data_40_correct.pkl')\n",
    "\n",
    "ori_patient_feature = dataset[0] #before cleaning, original data\n",
    "ori_label = dataset[1]\n",
    "ori_label_time = dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23160, (74, 48))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ori_label), ori_patient_feature[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Cleaning & train, val, test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#outlier removal\n",
    "list_length = []\n",
    "for i in range(1,101,1):\n",
    "    list_length.append(np.percentile(ori_label_time, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD8CAYAAACINTRsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122a56cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(list_length,bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_idx = np.where(ori_label_time <= list_length[-5])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patient_feature = ori_patient_feature[keep_idx]\n",
    "label = ori_label[keep_idx]\n",
    "label_time = ori_label_time[keep_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17787.2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(patient_feature)*0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split the dataset\n",
    "idx_arr = list(range(len(patient_feature)))\n",
    "np.random.shuffle(idx_arr)\n",
    "patient_feature_train, patient_feature_val, patient_feature_test = \\\n",
    "patient_feature[idx_arr[:15550]],patient_feature[idx_arr[15550:17788]],patient_feature[idx_arr[17788:]]\n",
    "label_train, label_val, label_test = \\\n",
    "label[idx_arr[:15550]],label[idx_arr[15550:17788]],label[idx_arr[17788:]]\n",
    "label_time_train, label_time_val, label_time_test = \\\n",
    "label_time[idx_arr[:15550]],label_time[idx_arr[15550:17788]],label_time[idx_arr[17788:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15550"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(patient_feature_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Helper function\n",
    "def block_tri(group_size,scale, num_units, mode):\n",
    "    '''\n",
    "    group_size: the size of each group\n",
    "    num_units: group_size x sum(scale)\n",
    "    mode: the way of connection, original, shift, fully connect\n",
    "    return: tensor mask\n",
    "    '''\n",
    "    mtrx = np.zeros((num_units, num_units))\n",
    "    if mode == 'original':\n",
    "        for i in range(int(num_units/group_size) - 1):\n",
    "            mtrx[i*group_size: , i*group_size:(i+1)*group_size] = 1\n",
    "    elif mode == 'shift': \n",
    "        mtrx = np.zeros((num_units, num_units))\n",
    "        for i in range(int(num_units/group_size)):\n",
    "            refer_li = sum([[i]*i for i in scale],[])\n",
    "            length = refer_li[i]\n",
    "            sequence = list(np.arange(i, num_units/group_size))\n",
    "            #print(sequence)\n",
    "            sequence = [int(j) for j in sequence if (j - i)%length == 0]\n",
    "            ##print(sequence)\n",
    "            for index in sequence:\n",
    "                mtrx[index*group_size:(index+1)*group_size,i*group_size:(i+1)*group_size] = 1\n",
    "    return mtrx \n",
    "\n",
    "#def activate_index()\n",
    "\n",
    "def activate_index(timestep, num_units, group_size, scale,index_li,batch_size,input_size = 48):\n",
    "    '''\n",
    "    timestep: the current timestep in a sequence\n",
    "    num_units: dimension of hidden layer\n",
    "    group_size: number of nodes in each group\n",
    "    scale: the range of update frequency\n",
    "    index_li: the index of each scale start point. dictionary. scale: position\n",
    "    input_size: the feature dim for patient\n",
    "    return: a matrix with 0 and 1. 1 for active rows seperately for linear layer h and i\n",
    "    '''\n",
    "    activation_map = np.zeros((batch_size, num_units))\n",
    "    for i in scale:\n",
    "        remain = timestep%i\n",
    "        if remain == 0:\n",
    "            index_temp = index_li[i]\n",
    "            activation_map[:,index_temp*group_size:(index_temp+1)*group_size] = np.ones((batch_size,group_size))\n",
    "        else:\n",
    "            index_temp = i - remain + index_li[i]\n",
    "            activation_map[:, index_temp*group_size:(index_temp+1)*group_size] = np.ones((batch_size,group_size))\n",
    "    return torch.from_numpy(activation_map).float()\n",
    "### make it a tensor\n",
    "\n",
    "def padding_fun(data, labels, label_time):\n",
    "    '''\n",
    "    This is going to pad the data in the front.\n",
    "    data: a batch of patient feature. now, a list (with length as batch_size) of array of T * 48\n",
    "    labels: a list of zero and one\n",
    "    return target: padded_data should be batch x 48 x max_length. called in the data loader function (this is different from context window since window has fixed size and can be done before)\n",
    "           length: original length of each patient records\n",
    "    '''\n",
    "    max_length = max(label_time)\n",
    "    target_data = np.array([np.pad(i,((max_length - len(i),0),(0,0)), 'constant', constant_values = 0) for i in data])\n",
    "    target_label = np.array([np.pad([labels[i]]*label_time[i],(max_length - label_time[i],0),'constant', constant_values = -1 ) for i in range(len(labels))])\n",
    "    #padding -1 here, so would be ignored when calculating loss\n",
    "    #print(data[i].shape)\n",
    "    target = np.array(data)\n",
    "    #print(target.shape)#for debug\n",
    "    return target_data, target_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#not used for now\n",
    "def activate_index_second(timestep, num_units, group_size, scale,index_li,batch_size,input_size = 48):\n",
    "    '''\n",
    "    timestep: the current timestep in a sequence\n",
    "    num_units: dimension of hidden layer\n",
    "    group_size: number of nodes in each group\n",
    "    scale: the range of update frequency\n",
    "    index_li: the index of each scale start point. dictionary. scale: position\n",
    "    input_size: the feature dim for patient\n",
    "    return: a matrix with 0 and 1. 1 for active rows seperately for linear layer h and i\n",
    "    '''\n",
    "    activation_map_h = np.zeros((num_units, num_units))\n",
    "    activation_map_i = np.zeros((input_size, num_units))\n",
    "    activation_map_h_re = np.ones(num_units)\n",
    "    #activation_map_i_re = np.ones(num_units)   \n",
    "\n",
    "    for i in scale:\n",
    "        remain = timestep%i\n",
    "        if remain == 0:\n",
    "            index_temp = index_li[i]\n",
    "            activation_map_h[:,index_temp*group_size:(index_temp+1)*group_size] = np.ones((batch_size,group_size))\n",
    "            activation_map_i[:,index_temp*group_size:(index_temp+1)*group_size] = np.ones((input_size,group_size))\n",
    "            activation_map_h_re[index_temp*group_size:(index_temp+1)*group_size] = np.zeros(group_size)\n",
    "            #activation_map_i_re[index_temp*group_size:(index_temp+1)*group_size] = np.zeros(group_size)\n",
    "        else:\n",
    "            index_temp = i - remain + index_li[i]\n",
    "            activation_map_h[:, index_temp*group_size:(index_temp+1)*group_size] = np.ones((batch_size,group_size))\n",
    "            activation_map_i[:, index_temp*group_size:(index_temp+1)*group_size] = np.ones((batch_size,group_size))\n",
    "            activation_map_h_re[index_temp*group_size:(index_temp+1)*group_size] = np.zeros(group_size)\n",
    "            #activation_map_i_re[index_temp*group_size:(index_temp+1)*group_size] = np.zeros(group_size)\n",
    "    return torch.from_numpy(activation_map_h).float(), torch.from_numpy(activation_map_i).float(),\\\n",
    "torch.from_numpy(activation_map_h_re).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.from_numpy(np.array([[1,2,3],[4,5,6]])).float()\n",
    "b = torch.from_numpy(np.ones(3)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([[1, 2, 3, 4, 5],[5,6,7,8,9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4, 5],\n",
       "       [5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(a, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.array([[1, 2, 3, 4, 5],[1,2,3,4,5]]).T\n",
    "b = np.array([[1, 2, 3, 4, 5,6],[1,2,3,4,5,7]]).T\n",
    "c = np.array([[1, 2, 3, 4],[1,2,3,4]]).T\n",
    "\n",
    "data = [a,b,c]\n",
    "labels = [1.0,1.0,1.0]\n",
    "label_time = [5.0, 6.0, 4.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_, label_ = padding_fun(data, labels, label_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "li = [k[0], k[1], k[2]]\n",
    "torch.stack(li, dim = 1).view(-1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = torch.Tensor([[1.0,1.0],[0.0,0.0],[1.0,1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.],\n",
       "       [ 0.,  0.],\n",
       "       [ 1.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target.transpose(0,1).contiguous().view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.stack([torch.Tensor(labels),torch.Tensor(label_time)],dim = 1).view(-1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class MIMICDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, label_list, label_time_li):\n",
    "        \"\"\"\n",
    "        @param data_list: list of datapoints, each element is a embedding matrix\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.label_list = label_list\n",
    "        self.label_time_list = label_time_li\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        return (self.data_list[key], self.label_list[key], self.label_time_list[key])\n",
    "\n",
    "#training data in this case should be an array of arrays. for each array, it is an array of size T*48 \n",
    "def MIMIC_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    label_time_list = []\n",
    "    for data_point in batch:#batch is a list of items(selected by index), each is imdb_train[i]#which is by above get item\n",
    "        data_list.append(data_point[0])\n",
    "        label_list.append(data_point[1])\n",
    "        label_time_list.append(data_point[2])\n",
    "\n",
    "    #return a batch of padded data\n",
    "    new_data, label_list = padding_fun(data_list, label_list, label_time_list) #should be batch x max_len x 48    batch x max_length\n",
    "    return [torch.from_numpy(new_data).float(),torch.from_numpy(np.array(label_list)).long()]\n",
    "\n",
    "def reload_data(batch_sz):\n",
    "    '''\n",
    "    batch_sz: pass in the batch size\n",
    "    return: data loader\n",
    "    ##TODO: no test loader has been said yet\n",
    "    '''        \n",
    "    #print(len(training_wds))\n",
    "    mimic_train = MIMICDataset(patient_feature_train, label_train, label_time_train)\n",
    "    mimic_val = MIMICDataset(patient_feature_val, label_val, label_time_val)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(dataset=mimic_train,batch_size=batch_sz,collate_fn=MIMIC_collate_func,shuffle=True)\n",
    "    validation_loader = torch.utils.data.DataLoader(dataset=mimic_train,batch_size=batch_sz,collate_fn=MIMIC_collate_func,shuffle=True)\n",
    "        \n",
    "    return train_loader, validation_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Model\n",
    "#forward\n",
    "#cell_class, step\n",
    "class Clock_NN(nn.Module):\n",
    "    def __init__(self, scale,batch_size, group_size = 1, activation_fun = F.relu, mean = 0, std = 0.1, input_dim = 48,mode = 'shift'):\n",
    "        super(Clock_NN, self).__init__()\n",
    "        '''\n",
    "        scale: the updating frequency, a list. [1,2,4,8,16,32]\n",
    "        batch_size: the size of batch\n",
    "        group_size: the number of nodes in each scale, default is 1.\n",
    "        activation_function\n",
    "        mean: the mean of Gaussian distribution for initialize weights for hidden layer\n",
    "        std: the standard devation of the Gaussian distribution for initialize weights for hidden layer\n",
    "        input_dim: the feature dimension of each time step\n",
    "        '''\n",
    "        self.scale = scale\n",
    "        self.group_size = group_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mode = mode\n",
    "        if mode == 'orginal':\n",
    "            self.num_units = self.scale*self.group_size\n",
    "        elif mode == 'shift':\n",
    "            self.num_units = sum(self.scale)*self.group_size\n",
    "            \n",
    "        self.class_dim = 2\n",
    "        self.input_dim = input_dim\n",
    "        self.linear_h = nn.Linear(self.num_units,self.num_units)\n",
    "        self.linear_o = nn.Linear(self.num_units,self.class_dim)\n",
    "        self.linear_i = nn.Linear(self.input_dim, self.num_units)\n",
    "        self.activation_fun = activation_fun\n",
    "        self.connect = torch.from_numpy(block_tri(self.group_size, self.scale, self.num_units, self.mode)).float()\n",
    "        self.time_step = 0\n",
    "        self.index_li = {i:i-1 for i in self.scale}\n",
    "        \n",
    "        self.initial_weights(mean, std)\n",
    "        \n",
    "    def forward(self, sequence, hidden):#depends on what passed for model.train(), to be filled)\n",
    "        '''\n",
    "        sequence: batch  x timestep x number_feature matrix\n",
    "        hidden: should be h0\n",
    "        '''     \n",
    "        #sequence = sequence.view(48,-1)when this is only one batch\n",
    "        hidden_output = []\n",
    "        length = sequence.size()[1]\n",
    "        logit = []\n",
    "        for i in range(1,length+1):\n",
    "            self.time_step += 1\n",
    "            #backwards, want discharge/dead time aligns\n",
    "            #print(sequence[:,:,-i].size())#would be batch*48\n",
    "            hidden = self.CW_RNN_Cell(sequence[:,-i,:].contiguous(), hidden)\n",
    "            hidden_output.append(hidden)#become batch_size x hidden_dim\n",
    "            out = self.linear_o(hidden)\n",
    "            #print(out.size())#should be batch x time x 2\n",
    "            logit.append(F.log_softmax(out))\n",
    "        return hidden_output, logit\n",
    "            \n",
    "                \n",
    "    def CW_RNN_Cell(self, x_input, hidden):\n",
    "        '''\n",
    "        x_input: number_feature x batch vector, representing one time stamp\n",
    "        hidden: output of the last cell, should be hidden_dim(i.e. num_units) x batch\n",
    "        '''\n",
    "        activate = activate_index(self.time_step, self.num_units, self.group_size, self.scale,self.index_li,batch_size, self.input_dim)\n",
    "        activate_re = torch.from_numpy(np.ones((self.batch_size,self.num_units))).float() - activate\n",
    "        \n",
    "        #print('activate ' + str(activate.size()))\n",
    "        #print('activate_re ' + str(activate_re.size()))\n",
    "        #print('hidden ' + str(hidden.size()))\n",
    "        #print('connect ' + str(self.connect.size()))\n",
    "        #print('input ' + str(x_input.size()))\n",
    "        #print('weight ' + str(self.linear_h.weight.size()))\n",
    "        #print('weight_type ' + str(type(self.linear_h.weight.data)))\n",
    "        #create the mask\n",
    "        #print(self.linear_h.weight.data)\n",
    "        self.linear_h.weight.data = self.linear_h.weight.data*(self.connect.transpose(0,1))#here needs transpose since previously left multiplication, activate mtrx doesn't need as rewrite and select cols.\n",
    "        self.linear_i.weight.data = self.linear_i.weight.data\n",
    "        \n",
    "        #print(self.linear_h.weight.data)\n",
    "        #*activate.transpose(0,1)\n",
    "        hidden_next = self.linear_h(hidden) + self.linear_i(x_input) #should be batch_size x hidden_dim\n",
    "        hidden_next.data = activate*hidden_next.data + activate_re*hidden.data\n",
    "        hidden_next = self.activation_fun(hidden_next)\n",
    "        return hidden_next\n",
    "\n",
    "    def init_hidden(self):\n",
    "        h0 = Variable(torch.zeros(self.batch_size,self.num_units))\n",
    "        return h0\n",
    "        \n",
    "    def initial_weights(self, mean, std):\n",
    "        lin_layers = [self.linear_h, self.linear_o, self.linear_i]\n",
    "        for layer in lin_layers:\n",
    "            layer.weight.data.normal_(mean, std**2)\n",
    "            layer.bias.data.fill_(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluation(data_loader, model):\n",
    "    '''\n",
    "    to be filled. use loader for easy call for summarizing training acc\n",
    "    '''\n",
    "    pre = []\n",
    "    labels = []\n",
    "    model.eval()\n",
    "    \n",
    "    for data, label in data_loader:\n",
    "        data, label = Variable(data), Variable(label)\n",
    "        #label_scores = model(data, label) #I guess should be a mtrx, batch*class_num?\n",
    "    \n",
    "        hidden= model.init_hidden()\n",
    "        hidden, output = model(data, hidden)\n",
    "\n",
    "        #now get a list of hidden and a list of outputs\n",
    "        label = label.transpose(0,1).contiguous().view(-1).data.numpy()\n",
    "        output = torch.stack(output, dim=1).view(-1, 2).data.numpy()\n",
    "        keep_idx = np.where(label != -1)[0]\n",
    "        label_new = list(label[keep_idx])\n",
    "        output_new = list(np.argmax(np.array(output[keep_idx]),axis = 1))\n",
    "        pre.extend(output_new)\n",
    "        labels.extend(label_new)\n",
    "        #print(len(label_new))\n",
    "        #print(label_new[20])\n",
    "        #print(output_new[20])\n",
    "        \n",
    "    #print(len(labels) == len(pre))\n",
    "    acc = sum(np.array(labels) == np.array(pre))/len(labels)\n",
    "    model.train()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 0.6914035677909851\n"
     ]
    }
   ],
   "source": [
    "### Training\n",
    "model = Clock_NN([1,2,4], batch_size, group_size = 1, activation_fun = F.relu, mean = 0, std = 0.1, input_dim = 48)\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss(ignore_index=-1)  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "accuracy_list = []\n",
    "train_loader, validation_loader = reload_data(batch_size)\n",
    "for epoch in range(num_epochs):\n",
    "    for step, (data, label) in enumerate(train_loader):        \n",
    "        data, label = Variable(data), Variable(label)\n",
    "        model.zero_grad()\n",
    "        hidden= model.init_hidden()\n",
    "        hidden, output = model(data, hidden)\n",
    "        \n",
    "        #now get a list of hidden and a list of outputs\n",
    "        label = label.transpose(0,1).contiguous().view(-1) \n",
    "        #should be flatten, batch_size x hidden. transpose due to below order, was batch, seq => follow up 2 down. get size batch*seq\n",
    "        output = torch.stack(output, dim=1).view(-1, 2) \n",
    "        #since batch is the first dimension, so dim is 1. now the order is  seq_len,batch, 2, so the first dimension is the first time step over all batch\n",
    "        #print(output.size())\n",
    "        #print(label.size())\n",
    "        lossy = loss(output, label)\n",
    "        lossy.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 20 == 0:\n",
    "            print(\"Epoch: {}; Loss: {}\".format(epoch, lossy.data[0]))\n",
    "            #print('accuracy_on_training: {}'.format(evaluation(train_loader))) \n",
    "            val_acc = evaluation(validation_loader, model)\n",
    "            print('accuracy_on_validation: {}'.format(val_acc)) \n",
    "            accuracy_list.append(val_acc)\n",
    "            if ((epoch > 5) and ((accuracy_list[-1] < (accuracy_list[-2] - 0.01)) or (accuracy_list[-1] < (accuracy_list[-3] - 0.01)))):\n",
    "                print(\"early stop, accuracy = \", accuracy_list[-2])\n",
    "                break\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "activate_index(7,14,2,[1,2,4],{1:0,2:1,4:3})\n",
    "\n",
    "a = torch.from_numpy(np.array([[1,2,3],[4,5,6]]))\n",
    "b = torch.from_numpy(np.array([[2,3,4],[4,5,6]]))\n",
    "c = a*b\n",
    "\n",
    "c\n",
    "\n",
    "c[-2,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "patient_feature[0]\n",
    "\n",
    "tryme2 = np.ones((5,4))\n",
    "\n",
    "tryme2\n",
    "\n",
    "tryme2[[2:4, ],2] = 0\n",
    "\n",
    "a = np.tril(np.ones((3,3))*(np.array([1,2,3]).reshape(3,1))).flatten()\n",
    "a[np.nonzero(a)]\n",
    "\n",
    "x = [[i]*i for i in [1,2,4]]\n",
    "\n",
    "x = sum(x,[])\n",
    "\n",
    "x\n",
    "\n",
    "4%3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mtrx\n",
    "\n",
    "block_tri(2,[1,2,4],14, mode = 'shift') == mtrx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "group = 2\n",
    "unit = 14\n",
    "mtrx = np.zeros((unit, unit))\n",
    "for i in range(int(unit/group)):\n",
    "    sequence = list(np.arange(i, unit/group))\n",
    "    #print(sequence)\n",
    "    sequence = [int(j) for j in sequence if (j - i)%x[i] == 0]\n",
    "    ##print(sequence)\n",
    "    for index in sequence:\n",
    "        mtrx[index*group:(index+1)*group,i*group:(i+1)*group] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "patient_feature = dataset[0]\n",
    "label = dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "label_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in list(sample):\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sample_tensor = Variable(torch.from_numpy(sample[0]).float())\n",
    "target_tensor = Variable(torch.from_numpy(label_sample[0]).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sample_tensor.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
